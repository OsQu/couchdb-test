\section{CouchDB}

CouchDB is a distributed database that provides eventual
consistency\cite{anderson2010couchdb}. It is designed around web technologies:
it stores its documents in JSON format and all the communication internally and
externally is done over HTTP\@. CouchDB is modeled around high availability
which categorizes it as AP system.

A CouchDB cluster consists of several identical nodes running CouchDB instances.
A traditional distributed architecture has a single master node that handles all
the writes the system. Leader node then replicates these writes to several
slave nodes that can also respond to read requests by clients. Master/slave
architecture does not scale well to large scale for two reasons. First, since
all the write requests go through a single node, that node need to have enough
processing power to handle all the request. Only way to scale to larger amount
of write requests is to scale vertically which gets very expensive for high-end
hardware. Second, master node acts as a single point of failure in the system.
When master node fails, the distributed system can not process any write
requests until it selects a new master.

Instead of master/slave architecture, each node in CouchDB cluster acts as an
independent instance that can handle reads and writes. This enables high
availability for the cluster: even if several nodes fail, the system is still
able to process requests. However as proven by CAP theorem, the high
availability comes with a cost: the system is no longer able to maintain
consistency between the nodes. This can affect user in certain ways, for example
user can write a document to the database and get \texttt{not\_found} error when
trying to read it moments later. This happens because database has not had time
to replicate the write operation to other node, which user used to read the
document.

Replication in CouchDB is done asynchronously. The replication can happen
periodically or after a replication command from the user depending on the
configuration. Upon replication CouchDB compares the two databases to find out
which documents on the source differ from the target and then sends these
changes as batches until all the changes are transferred.

To find out which documents has changed, databases in CouchDB have a sequence
number which is incremented every time the database is changed. That way CouchDB
is able to tell efficiently what happened between two sequences and can send
only changed data during replication.

The client receives a successful response from the server after a single node
have processed the query. It makes no guarantees that any other node have
received or processed the query because replication takes place later. This
makes both, processing the query and replication straight forward and provides
highly available system. However, because no guarantees about replication is
made, the system can lose all the updates after last replication if the data on
the single machine is corrupted for example due a hardware failure.

It is possible to model replication differently and still have an AP system.
Riak is a distributed database that is designed based on Amazon's Dynamo
architecture\cite{decandia2007dynamo}. Riak stores objects associated with a key
to the database. It uses consistent hashing\cite{karger1997consistent} to map
the keys to the data so that single machine can acts as many virtual nodes in
the hash ring. Riak replication can be configured with multiple factors. Riak
can be configured to replicate the data up to \texttt{N} nodes in the ring, and
clients can define value \texttt{R} upon read request. Riak returns the value
associated with the key when it receives a response from \texttt{R} nodes of
\texttt{N}. For write request the same value is called \texttt{W}. When the
client sends a write request, it can define to how many nodes Riak needs to
replicate the data until the request is considered successful. To simulate
CouchDB's behaviour, \texttt{R=W=1} could be used, but to achieve stronger
durability a greater value should be used.

When CouchDB replicates updates from one server to another, it is possible that
the same document was updated in both servers. The document can not be just
transferred because that would override the change that was made on the target
server. Instead, CouchDB marks the document as conflicted and deterministically
stores both versions in a way that they are same in both servers. Then the user
is required to resolve the conflict by either choosing one of the versions or
trying to merge them.

CouchDB detects conflicts between versions by comparing their \emph{revision
ids}. The database assigns revision id to each document it stores, and it is
increment by one each time the document updated. Because multiple servers can
update the same revision independently, the revision id also contains a MD5 hash
of the data stored in the document. If the documents have different revision ids
they are conflicting.\todo[inline]{Now there's plenty of weird stuff here: ID is
basically \texttt{<rev\_num>-MD5(document)}. Is the algorithm able to notice
consecutive edits that do not conflict and not mark them, what about emerging
branches with different revision numbers and conflicting data? Riak handles
these with dotted version vectors, Dynamo with vector clocks.}
\todo[inline]{Should MVCC be mentioned here?}

These problems are also present in other distributed databases that allow
updates to the documents without having a consistent state of the data. This is
often true with AP systems such as Dynamo or Riak.

Dynamo tracks changes to objects using vector clocks. Each time object is
updated, it is assigned with a vector clock associated with the client that made
the change. When the updates are done concurrently and they branch, the database
can figure out the causality between the changes and automatically apply
consecutive updates. It also notices conflicts between the versions, and can
present those to user for a semantic reconciliation.

A problem with vector clocks is that the size of the clocks might grow
unbounded. If the same object is accessed with many clients, the object gets
assigned with a vector clock for each client. This takes space and slows down
algorithm detecting conflicts. For that reason, Riak has changed its conflict
resolution algorithm to use dotted version vectors\cite{preguicca2010dotted}.
Dotted version vectors work very similarly than vector clocks, but they also
include a context (called dot) where the last update was made, and the system
can safely discard earlier vectors that were seen by all parties.

Usually distributed databases are designed to store extremely large amount of
data. They achieve this by dividing the database instances to smaller partitions
called clusters. Clusters are responsible of storing and managing a portions of
the data stored in the database.

As well as Riak and Dynamo, CouchDB uses consistent hashing to determine to
which shard it routes the request. CouchDB hashes the document ID with
CRC32-based hash algorithm, and looks up from hash ring to which server the
document is stored. The hash ring mapping is stored into each proxy that handles
the requests. Because it is static, it does not automatically adapt to the
changes in the hash topology, such as removal of the node. The documentation of
CouchDB also suggests that a tree of proxies should be used for very large
clusters. The database shards are replaced with intermediate proxies that
then connect to the database shards.\todo[inline]{Manually configuration sounds
very prone to human errors, can this be included here, albeit I don't have any
references to back up my gut feeling?}

Unlike Riak, CouchDB doesn't provide a way to employ virtual nodes. With virtual
nodes, the hash ring would be split into more nodes than there are physical
servers. Also the number of nodes could be changed, and consistent hashing takes
care that no more data is moved between physical nodes than necessary. This way
the data is distributed equally between physical nodes and the cluster could be
expanded just by adding virtual nodes.

CouchDB however embraces a technique called \emph{oversharding}. In oversharding
the cluster is partitioned so that there are multiple shards in the same
physical machine. When the size of a shard grows too large, it can be moved to
the another machine or split to smaller shards. This is similar to the virtual
nodes, but the amount of shards have to be guessed before hand, and moving or
splitting shards often require human intervention, although databases with
automatic sharding exists, such as MongoDB\@\footnote{Documentation about
MongoDB sharding, including automatic sharding can be found from
http://docs.mongodb.org/manual/sharding/}.
