\section{CouchDB}

CouchDB is a distributed database that provides eventual
consistency~\cite{anderson2010couchdb}. It is designed around web technologies:
it stores its documents in JSON format and all the communication internally and
externally is done over HTTP\@. CouchDB is modeled around high availability
which categorizes it as AP system.

A CouchDB cluster consists of several identical nodes running CouchDB instances.
A traditional distributed architecture has a single master node that handles all
the writes the system. Leader node then replicates these writes to several
slave nodes that can also respond to read requests by clients. Master/slave
architecture does not scale well to large scale for two reasons. First, since
all the write requests go through a single node, that node needs to have enough
processing power to handle all the request. Only way to scale to larger amount
of write requests is to scale vertically, which gets very expensive for high-end
hardware. Second, master node acts as a single point of failure in the system.
When master node fails, the distributed system can not process any write
requests until it selects a new master or the original master recovers from the
failure.

Instead of master/slave architecture, each node in CouchDB cluster acts as an
independent instance that can handle reads and writes. This enables high
availability for the cluster: even if several nodes fail, the system is still
able to process requests. However, as proven by CAP theorem, the high
availability comes with a cost: the system is no longer able to maintain
consistency between the nodes. This can affect user in certain ways, for example
user can write a document to the database and get \texttt{not\_found} error when
trying to read it moments later. This happens because database has not had time
to replicate the write operation to other node, which the user used to read the
document.

Replication in CouchDB is done asynchronously. The replication can happen
periodically or after a replication command from the user depending on the
configuration. Upon replication CouchDB compares the two databases to find out
which documents on the source differ from the target and then sends these
changes as batches until all the changes are transferred.

To find out which documents has changed, databases in CouchDB have a sequence
number which is incremented every time the database is changed. That way CouchDB
is able to tell efficiently what happened between two sequences and can send
only changed data during replication.

The client receives a successful response from the server after a single node
have processed the query. It makes no guarantees that any other node have
received or processed the query because replication takes place later. This
makes both, processing the query and replication straight forward and provides
highly available system. However, because no guarantees about replication is
made, the system can lose all the updates after last replication if the data on
the single machine is corrupted for example due a hardware failure.

It is possible to model replication differently and still have an AP system.
Riak is a distributed database that is designed based on Amazon's Dynamo
architecture~\cite{decandia2007dynamo}. Riak stores objects associated with a key
to the database. It uses consistent hashing~\cite{karger1997consistent} to map
the keys to the data so that single machine can acts as many virtual nodes in
the hash ring. Riak's replication is determined by multiple factors. Riak can be
configured to replicate the data up to \texttt{N} nodes in the ring, and clients
can define value \texttt{R} upon read request. Riak returns the value associated
with the key when it receives a response from \texttt{R} nodes out of \texttt{N}.
For write request the same value is called \texttt{W}. When the client sends a
write request, it can define to how many nodes Riak needs to replicate the data
to until the request is considered successful. To simulate CouchDB's behaviour,
\texttt{R=W=1} could be used, but higher values provider higher durability and
consistency.

When CouchDB database replicates updates from one server to another, it is
possible that the same document was updated in both servers. The document can
not be transferred because that would override the change that was made on the
target server. Instead, CouchDB marks the document as conflicted and
deterministically stores both versions in a way that their order is same in both
servers. Then the user is required to resolve the conflict by either choosing
one of the versions or trying to merge them.

CouchDB detects conflicts between versions by comparing \emph{revision ids} of
the documents. The database assigns revision id to each document it stores, and
it is incremented by one each time the document is updated. Because multiple
servers can update the same revision independently, the revision id also
contains a MD5 hash of the data stored in the document. If the documents have
different revision ids they are conflicting. CouchDB uses the hashes to resolve
the conflicts in a deterministic way. Each revision includes a list of previous
versions. First the database tries to choose a revision with a longer history
and if they are the same size, a revision id with higher ASCII sort order is
chosen.

These problems are also present in other distributed databases that allow
updates to the documents without having a consistent state of the data. This is
often true with AP systems such as Dynamo or Riak.

Dynamo tracks changes to objects using vector clocks. Each time object is
updated, it is assigned with a vector clock associated with the client that made
the change. When the updates are done concurrently and they branch, the database
can figure out the causality between the changes and automatically apply
consecutive updates. It also notices conflicts between the versions, and can
present those to user for a semantic reconciliation.

A problem with vector clocks is that the size of the clocks might grow
unbounded. If the same object is accessed with many clients, the object gets
assigned with a vector clock for each client. This takes space and slows down
algorithm detecting conflicts. For that reason, Riak has changed its conflict
resolution algorithm to use dotted version vectors~\cite{preguicca2010dotted}.
Dotted version vectors work very similarly than vector clocks, but they also
include a context (called dot) where the last update was made, and the system
can safely discard earlier vectors that were seen by all parties.

Usually distributed databases are designed to store extremely large amount of
data. They achieve this by dividing the database instances to smaller partitions
called clusters. Clusters are responsible of storing and managing a portions of
the data stored in the database.

At the time of writing, CouchDB does not offer any help with clustering the
database. Historically there were two projects that have tried to offer
clustering capabilities on top of CouchDB\@: CouchDB Lounge and BigCouch.
CouchDB Lounge was a standalone service that was deployed on top of CouchDB, and
it handled routing of operations to the correct nodes as well as load balancing
the system. Based on its GitHub page~\cite{couchdbloungegithub} it has not
received any maintenance since year 2010 but frankly, the documentation still
mentions and encourages the use of CouchDB Lounge.

Another approach to clustering was BigCouch. It was also a standalone service
but instead of proxying commands to CouchDB instances, it internally embedded
CouchDB and was distributed as a single package. BigCouch was modeled after
Amazon's Dynamo paper~\cite{decandia2007dynamo}, using similar consistent
hashing, and quorum protocols were applied to read and write operations.
BigCouch as a independent project was discountinued in January 2012, when
Cloudant, the company behind BigCouch, announced that it will be working with
CouchDB community to integrate core capabilities of BigCouch into a codebase of
CouchDB\@. In practice this meant a merge of these two projects and at the time
of writing, CouchDB does not offer any clustering capabilities so it is safe to
assume that merging of the projects is not ready.
